Architecture

SelfAttention(embed_size, num_heads):
    - split embeding into equal heads
    - linear layers for v,k,q equal mapping
    - linear out for entire embedding equal mapping

    - reshape v,k and q
    - energy with q,k
      (N, query_len, heads, head_dim) x (N, key_len, heads, head_dim)
      -> (N, heads, query_len, key_len)
    - mask if needed
    - attention (softmax)
    - output = (N, heads, query_len, key_len) x (N, value_len, heads, head_dim)
      -> (N, query_len, heads, head_dim)
    - concat all heads
    - run through linear equal mapping

TransformerBlock(embed_size, num_heads, dropout, forward_expansion):
    - attention (SelfAttention)
    - rescon -> layerNorm -> dropout
    - feedforward
    - rescon -> layerNorm -> dropout

Encoder(src_vocab_size, embed_size, num_layers, heads, device,
        forward_expansion, dropout, max_length)
    - word embedding
    - pos embedding
    - TransformerBlock x num_layers

    - pos embedding + word_embedding
    - dropout
    - for each layer perform comps

DecoderBlock(embed_size, forward_expansion, dropout, device)
    - attention with trg_mask
    - compute value
    - transformerBlock

Decoder(tgt_vocab_size, embed_size, num_layers, heads, forward_expansion
                        dropout, device, max_length)
    - DecoderBlock x numLayers

    - positions + word embeddings -> dropout
    - for each layer compute out
    - fc_out

Transformer(*)
    - make_src_mask
    - make_tgt_mask
    - run trough encoder
    - run through decoder
    - done
